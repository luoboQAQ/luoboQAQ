<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>交叉验证 on luoboQAQ</title><link>https://lbqaq.top/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/</link><description>Recent content in 交叉验证 on luoboQAQ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 10 May 2022 14:00:16 +0800</lastBuildDate><atom:link href="https://lbqaq.top/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/index.xml" rel="self" type="application/rss+xml"/><item><title>分类算法学习笔记</title><link>https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link><pubDate>Tue, 10 May 2022 14:00:16 +0800</pubDate><guid>https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid><description>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/86368113.webp" alt="Featured image of post 分类算法学习笔记" />&lt;h2 id="分类算法的作用">&lt;a href="#%e5%88%86%e7%b1%bb%e7%ae%97%e6%b3%95%e7%9a%84%e4%bd%9c%e7%94%a8" class="header-anchor">&lt;/a>分类算法的作用
&lt;/h2>&lt;p>分类是在一群已经知道类别标号的样本中，训练一种&lt;strong>分类器&lt;/strong>，让其能够对某种未知的样本进行分类。分类算法属于一种有监督的学习。分类算法的分类过程就是建立一种分类模型来描述预定的数据集或概念集，通过分析由属性描述的数据库元组来构造模型。分类的目的就是使用分类对新的数据集进行划分，其主要涉及分类规则的准确性、过拟合、矛盾划分的取舍等。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>有监督学习和无监督学习的区别&lt;/strong>
&lt;ul>
&lt;li>有监督学习是指数据集的正确输出已知情况下的一类学习算法。因为输入和输出已知，意味着输入和输出之间有一个关系，监督学习算法就是要发现和总结这种“关系”。&lt;/li>
&lt;li>无监督学习是指对无标签数据的一类学习算法。因为没有标签信息，意味着需要从数据集中发现和总结模式或者结构。&lt;/li>
&lt;li>&lt;strong>简单来说，是否有监督（supervised），就看输入数据是否有标签（label）&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="交叉验证">&lt;a href="#%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81" class="header-anchor">&lt;/a>交叉验证
&lt;/h2>&lt;h3 id="基本思想">&lt;a href="#%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3" class="header-anchor">&lt;/a>基本思想
&lt;/h3>&lt;p>交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组，一部分做为训练集(train set)，另一部分做为验证集(validation set or test set)，首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型(model)，以此来做为评价分类器的性能指标。&lt;strong>用交叉验证的目的是为了得到可靠稳定的模型&lt;/strong>。&lt;/p>
&lt;h3 id="k折交叉验证k-fold-cross-validation">&lt;a href="#k%e6%8a%98%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81k-fold-cross-validation" class="header-anchor">&lt;/a>K折交叉验证(K-fold cross-validation)
&lt;/h3>&lt;p>K折交叉验证就是进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练、测试评估，从而得出一个评价结果；如果是10折交叉验证，意思就是在原始数据集上，进行10次划分，每次划分进行一次训练、评估，最后得到10次划分后的评估结果，一般在这几次评估结果上取平均得到最后的评分。&lt;strong>其中，k一般取5或10。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>K折交叉验证的步骤：&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>将原始数据集划分为相等的K部分（“折”）&lt;/li>
&lt;li>将第1部分作为测试集，其余作为训练集&lt;/li>
&lt;li>训练模型，计算模型在测试集上的准确率&lt;/li>
&lt;li>每次用不同的部分作为测试集，重复步骤2和3 K次&lt;/li>
&lt;li>将平均准确率作为最终的模型准确率&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/1.png"
width="1102"
height="639"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/1_hu_4681689a69ccb62e.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/1_hu_b1b12769f774e315.png 1024w"
loading="lazy"
alt="10折交叉验证"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;blockquote>
&lt;p>💡 要会给定数据集进行K折交叉验证
要会计算模型准确率（每次的准确率、最终的准确率）&lt;/p>&lt;/blockquote>
&lt;h2 id="支持向量机svm">&lt;a href="#%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%basvm" class="header-anchor">&lt;/a>支持向量机(SVM)
&lt;/h2>&lt;h3 id="基本思想-1">&lt;a href="#%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3-1" class="header-anchor">&lt;/a>基本思想
&lt;/h3>&lt;p>SVM学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。&lt;/p>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/2.png"
width="922"
height="812"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/2_hu_3687cbff79f1d427.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/2_hu_85686e0bccfa57fc.png 1024w"
loading="lazy"
alt="SVM基本思想"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="272px"
>&lt;/p>
&lt;p>用我自己的理解，就是找一条线将两个数据集分开，要保证这条线到两边的数据集距离最大。那么，如何才能保证距离最大呢，这就是下面要讨论的难点了。&lt;/p>
&lt;h3 id="支持向量">&lt;a href="#%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f" class="header-anchor">&lt;/a>支持向量
&lt;/h3>&lt;blockquote>
&lt;p>在支持向量机中，距离超平面最近的且满足一定条件的几个训练样本点被称为支持向量。&lt;/p>&lt;/blockquote>
&lt;p>在上图中，处于虚线上的点（即红色的点）我们就将其定义为支持向量。那么，如何得到支持向量到超平面的距离呢？我们引入&lt;strong>几何间隔&lt;/strong>的概念：&lt;/p>
$$
\gamma = \frac{y(w^Tx + b)}{||w||_2} = \frac{\gamma^{'}}{||w||_2}
$$&lt;p>一般我们都取函数间隔$\gamma^{&amp;rsquo;}$为1，这样我们就可以得到支持向量到超平面的距离为$\frac{1}{||w||_2}$，两个支持向量之间的距离为$\frac{2}{||w||_2}$&lt;/p>
&lt;h3 id="svm模型目标函数">&lt;a href="#svm%e6%a8%a1%e5%9e%8b%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0" class="header-anchor">&lt;/a>SVM模型目标函数
&lt;/h3>&lt;p>SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：&lt;/p>
$$
max \\;\\; \frac{1}{||w||_2} \\;\\; s.t \\;\\; y_i(w^Tx_i + b) \geq 1 (i =1,2,...m)
$$&lt;p>其中，$||w||_2$为向量$w$的L2范数，即：&lt;/p>
$$
||w||_2=\sqrt{w_1^2+w_2^2}
$$&lt;p>为了去除根号方便计算，我们将原式转化为：&lt;/p>
$$
min \\;\\; \frac{1}{2}||w||_2^2 \\;\\; s.t \\;\\; y_i(w^Tx_i + b) \geq 1 (i =1,2,...m)
$$&lt;p>由于目标函数$\frac{1}{2}||w||_2^2$是凸函数，同时约束条件不等式是仿射的，根据凸优化理论，我们可以通过拉格朗日函数将我们的优化目标转化为无约束的优化函数。于是根据拉格朗日乘子法，我们得到：&lt;/p>
$$
L(w,b,\alpha) = \frac{1}{2}||w||_2^2 - \sum \limits _{i=1}^{m} \alpha_i [y_i(w^Tx_i + b) - 1] \\; 满足\alpha_i \geq 0
$$&lt;p>其中$\alpha_i$为拉格朗日乘子。&lt;/p>
&lt;h3 id="kkt条件">&lt;a href="#kkt%e6%9d%a1%e4%bb%b6" class="header-anchor">&lt;/a>KKT条件
&lt;/h3>&lt;p>我们对上面的式子求偏导，可以得到&lt;/p>
$$
\frac{\partial L }{\partial w}=0,\frac{\partial L }{\partial b}=0
$$&lt;p>解得&lt;/p>
$$
\boldsymbol{w}=\sum_{i=1}^N{\alpha_iy_i\boldsymbol{x}_{\boldsymbol{i}}}
$$$$
\sum_{i=1}^N{\alpha_iy_i}=0
$$&lt;p>解方程可得&lt;/p>
$$
\begin{cases}
\alpha_i\geq 0 \\\\
y_i(\overrightarrow{w_i}\cdot \overrightarrow{x_i}+b)-1\geq 0 \\\\
\alpha_i(y_i(\overrightarrow{w_i}\cdot\overrightarrow{x_i}+b)-1)=0
\end{cases}
$$&lt;p>上面的式子即为KKT条件&lt;/p>
&lt;h3 id="svm对偶性">&lt;a href="#svm%e5%af%b9%e5%81%b6%e6%80%a7" class="header-anchor">&lt;/a>SVM对偶性
&lt;/h3>&lt;p>现在我们令&lt;/p>
$$
\theta \left( \boldsymbol{w} \right) =\underset{\alpha _{_i}\ge 0}{\max}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right)
$$&lt;p>当样本点不满足约束条件时，即在可行解区域外$y_i\left(\boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b\right)&amp;lt;1$。此时，将$\alpha_i$设置为无穷大，则$\theta \left( \boldsymbol{w} \right)$也为无穷大。&lt;/p>
&lt;p>当样本点不满足约束条件时，即在可行解区域内$y_i\left(\boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b\right)\ge1$。此时，$\theta \left( \boldsymbol{w} \right)$为原函数本身。&lt;/p>
&lt;p>于是，将两种情况合并起来就可以得到我们新的目标函数：&lt;/p>
$$
\theta \left( \boldsymbol{w} \right) =\begin{cases} \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2\ ,\boldsymbol{x}\in \text{可行区域}\\\\ +\infty \ \ \ \ \ ,\boldsymbol{x}\in \text{不可行区域}\\\\ \end{cases}
$$&lt;p>于是原约束问题就等价于：&lt;/p>
$$
\underset{\boldsymbol{w,}b}{\min}\ \theta \left( \boldsymbol{w} \right) =\underset{\boldsymbol{w,}b}{\min}\underset{\alpha _i\ge 0}{\max}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right)
$$&lt;p>由上小节，我们可以知道该函数满足拉格朗日函数&lt;strong>对偶性&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>优化问题是凸优化问题&lt;/li>
&lt;li>满足KKT条件&lt;/li>
&lt;/ul>
&lt;p>于是，我们可以将其最小和最大的位置交换一下，这样就变成了：&lt;/p>
$$
\underset{\alpha _i\ge 0}{\max}\underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right)
$$&lt;p>从上式中，我们可以先求优化函数对于$w$和$b$的极小值。接着再求拉格朗日乘子$\alpha$的极大值。&lt;/p>
&lt;p>通过在上节中由偏导推出的两个式子，我们得到了$w$和$\alpha$的关系，就可以带入优化函数$L(w,b,\alpha)$消去$w$了&lt;/p>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code1.png"
width="583"
height="76"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code1_hu_cb82dee7ae24659e.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code1_hu_c187ca30af0efd45.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="767"
data-flex-basis="1841px"
>&lt;/p>
&lt;p>此时原式为&lt;/p>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code2.png"
width="457"
height="76"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code2_hu_d251def9ea181a2.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code2_hu_7a23d688c46e7d21.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="601"
data-flex-basis="1443px"
>&lt;/p>
&lt;p>我们对目标式子加一个负号，将求解极大转换为求解极小&lt;/p>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code3.png"
width="422"
height="76"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code3_hu_588c87eed60ed567.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/Code3_hu_6db0bce3cc74ddaa.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="555"
data-flex-basis="1332px"
>&lt;/p>
&lt;h2 id="svm核kernel方法">&lt;a href="#svm%e6%a0%b8kernel%e6%96%b9%e6%b3%95" class="header-anchor">&lt;/a>SVM核（Kernel）方法
&lt;/h2>&lt;h3 id="基本概念">&lt;a href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5" class="header-anchor">&lt;/a>基本概念
&lt;/h3>&lt;p>在上节我们推导出的式子中，我们可以看到上式低维特征仅仅以内积$x_i \bullet x_j$ 的形式出现，如果我们定义一个低维特征空间到高维特征空间的映射$T$，将所有特征映射到一个更高的维度，让数据线性可分，我们就可以求出分离超平面和分类决策函数了。&lt;/p>
&lt;p>但是，将数据从低维映射到高维，将会大大增加计算的复杂度，如果遇到无穷维的情况，就根本无从计算了。这时，就需要引入&lt;strong>核函数&lt;/strong>了。&lt;/p>
&lt;p>我们定义果存在函数$K(x,z)$，对于任意$x, z$ ，都有：&lt;/p>
$$
K(x, z) = T(x) \bullet T(z)
$$&lt;p>那么我们就称$K(x, z)$为核函数。&lt;/p>
&lt;p>通过核函数，就避免了在刚才我们提到了在高维维度空间计算内积的恐怖计算量。也就是说，我们可以好好享受在高维特征空间线性可分的红利，却避免了高维特征空间恐怖的内积计算量。&lt;/p>
&lt;p>下面我们来看看常见的核函数, 选择这几个核函数介绍是因为scikit-learn中默认可选的就是下面几个核函数。&lt;/p>
&lt;h3 id="线性核函数linear-kernel">&lt;a href="#%e7%ba%bf%e6%80%a7%e6%a0%b8%e5%87%bd%e6%95%b0linear-kernel" class="header-anchor">&lt;/a>线性核函数（Linear Kernel）
&lt;/h3>&lt;p>其实就是线性可分SVM，表达式为：&lt;/p>
$$
K(x, z) = x \bullet z
$$&lt;p>也就是说，线性可分SVM我们可以和线性不可分SVM归为一类，区别仅仅在于线性可分SVM用的是线性核函数。&lt;/p>
&lt;h3 id="多项式核函数polynomial-kernel">&lt;a href="#%e5%a4%9a%e9%a1%b9%e5%bc%8f%e6%a0%b8%e5%87%bd%e6%95%b0polynomial-kernel" class="header-anchor">&lt;/a>多项式核函数（Polynomial Kernel）
&lt;/h3>&lt;p>多项式核函数是线性不可分SVM常用的核函数之一，表达式为：&lt;/p>
$$
K(x, z) = （\gamma x \bullet z + r)^d
$$&lt;p>其中，$\gamma, r,d$都需要自己调参定义。&lt;/p>
&lt;h3 id="高斯核函数gaussian-kernel">&lt;a href="#%e9%ab%98%e6%96%af%e6%a0%b8%e5%87%bd%e6%95%b0gaussian-kernel" class="header-anchor">&lt;/a>高斯核函数（Gaussian Kernel）
&lt;/h3>&lt;p>在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是非线性分类SVM最主流的核函数。libsvm默认的核函数就是它。表达式为：&lt;/p>
$$
K(x, z) = exp(-\gamma||x-z||^2)
$$&lt;p>其中，$\gamma$大于0，需要自己调参定义。&lt;/p>
&lt;blockquote>
&lt;p>💡 高斯核函数很重要，一定要记住&lt;/p>&lt;/blockquote>
&lt;h3 id="sigmoid核函数">&lt;a href="#sigmoid%e6%a0%b8%e5%87%bd%e6%95%b0" class="header-anchor">&lt;/a>Sigmoid核函数
&lt;/h3>&lt;p>也是线性不可分SVM常用的核函数之一，表达式为：&lt;/p>
$$
K(x, z) = tanh（\gamma x \bullet z + r)
$$&lt;p>其中，$\gamma, r$都需要自己调参定义。&lt;/p>
&lt;h2 id="svm软间隔">&lt;a href="#svm%e8%bd%af%e9%97%b4%e9%9a%94" class="header-anchor">&lt;/a>SVM软间隔
&lt;/h2>&lt;p>有时候本来数据的确是可分的，也就是说可以用 线性分类SVM的学习方法来求解，但是却因为混入了异常点，导致不能线性可分，比如下图&lt;/p>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/3.png"
width="300"
height="245"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/3_hu_e4d495a9e40656e.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/3_hu_247b535caae0e6fd.png 1024w"
loading="lazy"
alt="有异常点的数据"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="293px"
>&lt;/p>
&lt;p>这种情况下，SVM引入了软间隔最大化的方法来解决。&lt;/p>
&lt;p>SVM对训练集里面的每个样本$(x_i,y_i)$引入了一个松弛变量$\xi_i \geq 0$,使函数间隔加上松弛变量大于等于1，也就是说：&lt;/p>
$$
y_i(w\bullet x_i +b) \geq 1- \xi_i
$$&lt;p>对比硬间隔最大化，可以看到我们对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，松弛变量不能白加，这是有成本的，每一个松弛变量$\xi_i$, 对应了一个代价$\xi_i$，这个就得到了我们的软间隔最大化的SVM学习条件如下：&lt;/p>
$$
min\\;\\; \frac{1}{2}||w||_2^2 +C\sum\limits{i=1}^{m}\xi_i
$$$$
s.t. \\;\\; y_i(w^Tx_i + b) \geq 1 - \xi_i \\;\\;(i =1,2,...m)
$$$$
\xi_i \geq 0 \\;\\;(i =1,2,...m)
$$&lt;p>这里，$C&amp;gt;0$为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。$C$越大，对误分类的惩罚越大，$C$越小，对误分类的惩罚越小。&lt;/p>
&lt;p>也就是说，我们希望$\frac{1}{2}||w||_2^2$尽量小，误分类的点尽可能的少。C是协调两者关系的正则化惩罚系数。在实际应用中，需要调参来选择。&lt;/p>
&lt;h2 id="svm编程实现">&lt;a href="#svm%e7%bc%96%e7%a8%8b%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>SVM编程实现
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;span class="lnt">72
&lt;/span>&lt;span class="lnt">73
&lt;/span>&lt;span class="lnt">74
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-matlab" data-lang="matlab">&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 生成测试数据&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">classCount&lt;/span>&lt;span class="p">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span>&lt;span class="p">=&lt;/span> &lt;span class="mi">100&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">bound&lt;/span> &lt;span class="p">=[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">data&lt;/span> &lt;span class="p">=[&lt;/span>&lt;span class="c">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="nb">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="nb">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="c">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="nb">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="nb">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="c">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span>&lt;span class="p">=&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="k">end&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Y&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="k">end&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 显示生成的测试数据&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">figure&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">clf&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gscatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">Y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;rb&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;X+&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">legend&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="s">&amp;#39;分类-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;分类+1&amp;#39;&lt;/span>&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">title&lt;/span>&lt;span class="p">(&lt;/span>&amp;#34;原始数据散点图&amp;#34;&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xlim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ylim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 拆分训练集和测试集&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">trainIndex&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nb">mod&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">testIndex&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="o">~&lt;/span>&lt;span class="n">trainIndex&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Xtrain&lt;/span>&lt;span class="p">=&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trainIndex&lt;/span>&lt;span class="p">,:);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Ytrain&lt;/span>&lt;span class="p">=&lt;/span>&lt;span class="n">Y&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">trainIndex&lt;/span>&lt;span class="p">,:);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Xtest&lt;/span>&lt;span class="p">=&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">testIndex&lt;/span>&lt;span class="p">,:);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Ytest&lt;/span>&lt;span class="p">=&lt;/span>&lt;span class="n">Y&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">testIndex&lt;/span>&lt;span class="p">,:);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 将拆分的结果展示&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">figure&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">clf&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hold&lt;/span> &lt;span class="n">on&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gscatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Xtrain&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">Xtrain&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">Ytrain&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;rb&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;X+&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Xtest&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">Xtest&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="s">&amp;#39;k.&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hold&lt;/span> &lt;span class="n">off&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">legend&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="s">&amp;#39;分类-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;分类+1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;测试数据&amp;#39;&lt;/span>&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">title&lt;/span>&lt;span class="p">(&lt;/span>&amp;#34;划分测试数据后的散点图&amp;#34;&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xlim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ylim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 使用SVM进行训练&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">svm&lt;/span>&lt;span class="p">=&lt;/span>&lt;span class="n">fitcsvm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Xtrain&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">Ytrain&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 展示SVM训练的结果&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">figure&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">clf&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gscatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Xtrain&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">Xtrain&lt;/span>&lt;span class="p">(:,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">Ytrain&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;rb&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;X+&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hold&lt;/span> &lt;span class="n">on&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gscatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectors&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectors&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectorLabels&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;rb&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;oo&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">13&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xg&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">bound&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">yg&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Bias&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">bound&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Bias&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;Linewidth&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">delta&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Bias&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">Beta&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yg&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;--g&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;LineWidth&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yg&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;--g&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;LineWidth&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hold&lt;/span> &lt;span class="n">off&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xlim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ylim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">legend&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="s">&amp;#39;分类-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;分类+1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;支持向量-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;支持向量+1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;超平面&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;超平面-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;超平面+1&amp;#39;&lt;/span>&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">title&lt;/span>&lt;span class="p">(&lt;/span>&amp;#34;&lt;span class="n">SVM训练后的散点图&lt;/span>&amp;#34;&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 使用训练好的模型进行预测&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Ypred&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Xtest&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 绘制预测后的结果&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">figure&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">clf&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gscatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Xtest&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">Xtest&lt;/span>&lt;span class="p">(:,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">Ypred&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;rb&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;X+&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hold&lt;/span> &lt;span class="n">on&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">g&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="n">gscatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectors&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectors&lt;/span>&lt;span class="p">(:,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">svm&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">SupportVectorLabels&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;rb&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;oo&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">13&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;Linewidth&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yg&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;--g&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;Linewidth&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yg&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;--g&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;Linewidth&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hold&lt;/span> &lt;span class="n">off&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">xlim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ylim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bound&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">legend&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="s">&amp;#39;分类-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;分类+1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;支持向量-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;支持向量+1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;超平面&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;超平面-1&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#39;超平面+1&amp;#39;&lt;/span>&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">title&lt;/span>&lt;span class="p">(&lt;/span>&amp;#34;&lt;span class="n">SVM预测后的散点图&lt;/span>&amp;#34;&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">%% 计算误差&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">fprintf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;正确率为：%.2f%%\n&amp;#39;&lt;/span>&lt;span class="p">,(&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Ypred&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="n">Ytest&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nb">numel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Ytest&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.*&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/4.png"
width="700"
height="521"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/4_hu_ad6b3e3724079201.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/4_hu_661d237d39149284.png 1024w"
loading="lazy"
alt="原始数据散点图"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="322px"
>&lt;/p>
&lt;p>&lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/5.png"
width="700"
height="520"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/5_hu_9e77e1b3d168e3a9.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/5_hu_db2f9cbfca0dd610.png 1024w"
loading="lazy"
alt="SVM训练后的散点图"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="323px"
> &lt;img src="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/6.png"
width="700"
height="527"
srcset="https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/6_hu_8447ac52b2d0c98d.png 480w, https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/IMAGE/6_hu_4c45fd3b17ecc87e.png 1024w"
loading="lazy"
alt="SVM预测后的散点图"
class="gallery-image"
data-flex-grow="132"
data-flex-basis="318px"
>&lt;/p>
&lt;h2 id="svm实现多分类">&lt;a href="#svm%e5%ae%9e%e7%8e%b0%e5%a4%9a%e5%88%86%e7%b1%bb" class="header-anchor">&lt;/a>SVM实现多分类
&lt;/h2>&lt;p>SVM算法最初是为二值分类问题设计的，当处理多类问题时，就需要构造合适的多类分类器。&lt;/p>
&lt;p>目前，构造SVM多类分类器的方法主要有两类：&lt;/p>
&lt;p>（1）直接法，直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。这种方法看似简单，但其计算复杂度比较高，实现起来比较困难，只适合用于小型问题中；&lt;/p>
&lt;p>（2）间接法，主要是通过组合多个二分类器来实现多分类器的构造，常见的方法有one-against-one和one-against-all两种。&lt;/p>
&lt;h3 id="一对多法ovr">&lt;a href="#%e4%b8%80%e5%af%b9%e5%a4%9a%e6%b3%95ovr" class="header-anchor">&lt;/a>一对多法（OVR）
&lt;/h3>&lt;p>训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。分类时将未知样本分类为具有最大分类函数值的那类。&lt;/p>
&lt;p>假如我有四类要划分（也就是4个Label），他们是A、B、C、D。&lt;/p>
&lt;p>于是我在抽取训练集的时候，分别抽取&lt;/p>
&lt;ol>
&lt;li>A所对应的向量作为正集，B，C，D所对应的向量作为负集；&lt;/li>
&lt;li>B所对应的向量作为正集，A，C，D所对应的向量作为负集；&lt;/li>
&lt;li>C所对应的向量作为正集，A，B，D所对应的向量作为负集；&lt;/li>
&lt;li>D所对应的向量作为正集，A，B，C所对应的向量作为负集；&lt;/li>
&lt;/ol>
&lt;p>使用这四个训练集分别进行训练，然后的得到四个训练结果文件。在测试的时候，把对应的测试向量分别利用这四个训练结果文件进行测试。最后每个测试都有一个结果f1(x),f2(x),f3(x),f4(x)。&lt;/p>
&lt;p>于是最终的结果便是这四个值中最大的一个作为分类结果。&lt;/p>
&lt;h3 id="一对一法ovo">&lt;a href="#%e4%b8%80%e5%af%b9%e4%b8%80%e6%b3%95ovo" class="header-anchor">&lt;/a>一对一法（OVO）
&lt;/h3>&lt;p>一般都是用一对多法，这里就不展开了，知道有这个东西就行。&lt;/p>
&lt;h2 id="参考资料">&lt;a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-anchor">&lt;/a>参考资料
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/31886934" target="_blank" rel="noopener"
>支持向量机（SVM）——原理篇 - 知乎&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/pinard/p/6097604.html" target="_blank" rel="noopener"
>支持向量机原理(一) 线性支持向量机 - 刘建平Pinard - 博客园&lt;/a> 等同系列文章&lt;/li>
&lt;li>&lt;a class="link" href="https://www.bilibili.com/video/BV13r4y1z7AG/" target="_blank" rel="noopener"
>【数之道25】机器学习必经之路-SVM支持向量机的数学精华_哔哩哔哩_bilibili&lt;/a> 等同系列视频&lt;/li>
&lt;/ul></description></item></channel></rss>