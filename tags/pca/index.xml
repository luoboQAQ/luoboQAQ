<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PCA on luoboQAQ</title>
    <link>https://lbqaq.top/tags/pca/</link>
    <description>Recent content in PCA on luoboQAQ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 17 May 2022 22:45:18 +0800</lastBuildDate><atom:link href="https://lbqaq.top/tags/pca/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>数据可视化笔记</title>
      <link>https://lbqaq.top/p/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 17 May 2022 22:45:18 +0800</pubDate>
      
      <guid>https://lbqaq.top/p/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AC%94%E8%AE%B0/</guid>
      <description>主成分分析(PCA) 基本思想 PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据$(x^{(1)},x^{(2)},&amp;hellip;,x^{(m)})$。我们希望将这m个数据的维度从n维降到n&amp;rsquo;维，希望这m个n&amp;rsquo;维的数据集尽可能的代表原始数据集。
通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。
由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法。
特征值分解(EVD) 特征值和特征向量的定义如下：
$$ Ax=\lambda x $$
其中A是一个$n \times n$的实对称矩阵，$x$是一个$n$维向量，则我们说$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$所对应的特征向量。
求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的$n$个特征值$\lambda_1 \leq \lambda_2 \leq &amp;hellip; \leq \lambda_n$,以及这$n$个特征值所对应的特征向量${q_1,q_2,&amp;hellip;q_n}$，如果这$n$个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：$A=Q\Sigma Q^{-1}$ 。
其中，Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵A的信息可以由其特征值和特征向量表示。
注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。
奇异值分解(SVD) SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个$m \times n$的矩阵，那么我们定义矩阵A的SVD为：
$$ A = U\Sigma V^T $$
其中U是一个$m \times m$的矩阵，$\Sigma$是一个$m \times n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个$n \times n$的矩阵。
计算方法：
 我们将A的转置和A做矩阵乘法，那么会得到$n \times n$的一个方阵$A^TA$。对其进行特征值分解，得到n个特征值和对应的n个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n \times n$的矩阵V，就是我们SVD公式里面的V矩阵了。 对$AA^T$进行特征值分解，得到m个特征值和对应的m个特征向量$u$了。将$AA^T$的所有特征向量张成一个$m \times m$的矩阵U，就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。 由于奇异值矩阵$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\sigma$就可以了。求解奇异值可以用$\sigma_i = Av_i / u_i$或$\sigma_i = \sqrt{\lambda_i}$  算法过程 输入：n维样本集$D=(x^{(1)}, x^{(2)},&amp;hellip;,x^{(m)})$，要降维到的维数n&#39;.
输出：降维后的样本集$D&#39;$
 对所有的样本进行中心化： $x^{(i)} = x^{(i)} - \frac{1}{m}\sum\limits_{j=1}^{m} x^{(j)}$ 计算样本的协方差矩阵$XX^T$ 对矩阵$XX^T$进行特征值分解 取出最大的n&amp;rsquo;个特征值对应的特征向量$(w_1,w_2,&amp;hellip;,w_{n&amp;rsquo;})$, 将所有的特征向量标准化后，组成特征向量矩阵W。 对样本集中的每一个样本$x^{(i)}$,转化为新的样本$z^{(i)}=W^Tx^{(i)}$ 得到输出样本集$D&amp;rsquo; =(z^{(1)}, z^{(2)},&amp;hellip;,z^{(m)})$  上面是采用EVD，如果采用SVD，在第二第三步时就用SVD进行解决。</description>
    </item>
    
  </channel>
</rss>
