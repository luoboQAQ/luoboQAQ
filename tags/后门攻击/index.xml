<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>后门攻击 on luoboQAQ</title>
        <link>https://lbqaq.top/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/</link>
        <description>Recent content in 后门攻击 on luoboQAQ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 13 Sep 2023 16:38:20 +0800</lastBuildDate><atom:link href="https://lbqaq.top/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Graph Unlearning</title>
        <link>https://lbqaq.top/p/graph-unlearning/</link>
        <pubDate>Wed, 13 Sep 2023 16:38:20 +0800</pubDate>
        
        <guid>https://lbqaq.top/p/graph-unlearning/</guid>
        <description>&lt;img src="https://lbqaq.top/p/graph-unlearning/110091745.webp" alt="Featured image of post Graph Unlearning" /&gt;&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;数据保护最近引起了越来越多的关注，并且已经提出了一些法规来保护个人用户的隐私。在这些法规中，提到了「被遗忘权」，它赋予数据主体从存储数据的实体中删除其数据的权利。这也意味着在机器学习中，模型提供者有义务消除其所有者要求被遗忘的数据的任何影响，即遗忘学习。&lt;/p&gt;
&lt;p&gt;最简单和有效的遗忘学习方法就是移除对应的样本后重新训练模型，然而当底层数据集很大时，这种方法在计算上可能令人望而却步。目前通用的遗忘学习方法是SISA (Sharded, Isolated, Sliced, and Aggregated) ——将训练集分为shards, shards中分为slices, 对于每个slice训练之后记录model parameters,  每个数据点被划分到不同的shards和slices中, unlearn时就是排除掉对应数据点然后retrain对应的shard和slices,  以空间开销换取训练的时间开销。&lt;/p&gt;
&lt;p&gt;对于图像和文本数据，分割数据没有什么问题。然而，对于图来说，GNN依赖于图结构信息，像在SISA中那样将节点随机划分为子图可能会严重损坏生成的模型。对此，作者提出了GraphEraser，以实现GNN中的遗忘学习。&lt;/p&gt;
&lt;p&gt;作者将图遗忘学习分为node unlearning「节点遗忘学习」和edge unlearning「边遗忘学习」，提出了两种图分割策略。第一种侧重于graph structural information「图结构信息」，另一种则是同时考虑graph structural and node feature information「图结构和节点特征信息」。&lt;/p&gt;
&lt;p&gt;为了同时考虑图结构和节点特征信息，作者将节点特征和图结构转化为嵌入向量，然后将其聚类为不同的shards。但是由于现实世界图的结构特性，传统的群落检测和聚类方法划分会导致分片大小不平衡，而大部分需要被撤销的数据都在最大的分区，从而导致效率低下。作者提出了两种分割算法和一种聚合算法以解决此问题。&lt;/p&gt;
&lt;h3 id=&#34;贡献&#34;&gt;贡献&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;第一次提出了在GNN模型上的遗忘学习方法&lt;/li&gt;
&lt;li&gt;提出了两种算法以平衡图分割块大小&lt;/li&gt;
&lt;li&gt;提出了一种基于学习的聚合方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;问题构造&#34;&gt;问题构造&lt;/h2&gt;
&lt;h3 id=&#34;问题定义&#34;&gt;问题定义&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;节点遗忘学习&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于GNN模型$F_o$，每个数据主体的数据对应于GNN训练图$G_o$中的一个节点。数据主体$u$要删除其所有数据，则意味着从GNN的训练图中遗忘学习$u$的节点特征以及其于其他节点的链接。以社交网络为例，节点遗忘学习意味着需要从目标GNN的训练图中删除用户的个人资料信息和社交关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;边遗忘学习&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;数据主体$u$要删除其节点于另一个节点$v$之间的一条边缘。仍然以社交网络为例，边缘遗忘意味着社交网络用户想要隐藏他们与另一个人的关系。&lt;/p&gt;
&lt;h3 id=&#34;评估指标&#34;&gt;评估指标&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;unlearning efficiency「遗忘学习效率」：与在训练的时间有关，时间要尽可能短。&lt;/li&gt;
&lt;li&gt;model utility「模型效用」：与准确性有关，越高越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在之前提过，图分割存在分片大小不均匀的问题。为此，作者提出了两种分片目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G1: Balanced Shards「均衡分片」：每个分片中的节点数量相似。这样，每个分片的再训练时间是相似的，从而提高了整个图遗忘学习过程的效率。&lt;/li&gt;
&lt;li&gt;G2: Comparable Model Utility「可比模型效用」：图结构信息是决定GNN性能的主要因素，每个分片都应保留图的结构属性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;grapheraser框架构造&#34;&gt;GraphEraser框架构造&lt;/h3&gt;
&lt;p&gt;作者将GraphEraser框架分为三个阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balanced Graph Partition「平衡图分区」：将训练图划分为不相交的分片&lt;/li&gt;
&lt;li&gt;Shard Model Training「分片训练模型」：对每个分片进行训练一个模型，称之为shard model「分片模型」$F_i$&lt;/li&gt;
&lt;li&gt;Shard Model Aggregation「分片模型聚合」：为了得到预测节点$w$的标签，将对应的数据（$w$的特征、其邻居的特征以及其中的图结构）同时发送到所有分片模型，并通过聚合所有分片模型的预测来获得最终预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GraphEraser框架的结构图如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230910155749458.png&#34;
	width=&#34;1609&#34;
	height=&#34;546&#34;
	srcset=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230910155749458_hud39e8982ef73543d3f849bdec9eb1737_175291_480x0_resize_box_3.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230910155749458_hud39e8982ef73543d3f849bdec9eb1737_175291_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GraphEraser框架图&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;294&#34;
		data-flex-basis=&#34;707px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;平衡图分割&#34;&gt;平衡图分割&lt;/h2&gt;
&lt;p&gt;作者提出了三种图分区策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略0&lt;/strong&gt;：仅考虑节点特征信息，并随机对节点进行分区&lt;/p&gt;
&lt;p&gt;该策略可以满足G1「均衡分片」要求，但不满足G2「可比模型效用」要求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略1&lt;/strong&gt;：依靠community detection「社区发现」，仅考虑结构信息，并尽可能保留它&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;策略2&lt;/strong&gt;：同时考虑结构信息和节点特征。将节点特征和图结构表示为低维向量，即节点嵌入，然后将节点嵌入聚类到不同的分片中。&lt;/p&gt;
&lt;p&gt;直接这样划分会导致划分区域不平衡，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911095231520.png&#34;
	width=&#34;970&#34;
	height=&#34;376&#34;
	srcset=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911095231520_hu5712cd4b4913b0734a11331a3243a00e_32481_480x0_resize_box_3.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911095231520_hu5712cd4b4913b0734a11331a3243a00e_32481_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;在Cora数据集上使用传统划分区域的大小&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;257&#34;
		data-flex-basis=&#34;619px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来，作者便介绍了对应的平衡图分区算法。&lt;/p&gt;
&lt;h3 id=&#34;社区发现算法&#34;&gt;社区发现算法&lt;/h3&gt;
&lt;p&gt;对于策略1，主要依赖的就是此算法。作者基于Label Propagation Algorithm (LPA)「标签传播算法」来设计图分区算法。在本文中，shard就是community。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;标签传播算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911105605800.png&#34;
	width=&#34;898&#34;
	height=&#34;358&#34;
	srcset=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911105605800_hu3935ab8e67fb5b75cf4b990ba093b12e_53628_480x0_resize_box_3.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911105605800_hu3935ab8e67fb5b75cf4b990ba093b12e_53628_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LPA工作流程&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;602px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在初始阶段（图a），每个节点都随机分配一个分片标签。&lt;/p&gt;
&lt;p&gt;在标签传播阶段（图b → 图 c），每个节点都会发送自己的标签，将自己更新成收到最多的那个标签&lt;/p&gt;
&lt;p&gt;标签传播过程会对所有节点进行多次迭代，直到收敛（没有节点更改标签）&lt;/p&gt;
&lt;p&gt;就如之前提到的，传统的LPA会导致高度不平衡的图形分区，严重影响了遗忘学习的效率。&lt;/p&gt;
&lt;p&gt;对此，作者提出了一个实现平衡图分区的&lt;strong&gt;一般方法&lt;/strong&gt;。给定所需的分片大小$k$和最大分片大小$\delta$，为每个节点-分片定义一个可能被分配到此分片的preference「偏好值」，代表该节点被分配给了分片（这被称为destination shard「目标分片」），从而产生$k \times n$个偏好值。对这些值进行排序，如果目标分片中的节点数不超过$\delta$，就将该节点分配给此分片。&lt;/p&gt;
&lt;p&gt;具体而言，作者提出了Balanced LPA (BLPA)「平衡标签传播算法」，将偏好值定义为节点分片对的neighbor counts「邻居计数」（属于目标分片的邻居数量），并且具有较大邻居计数的节点分片对具有更高的优先级分配。&lt;/p&gt;
&lt;p&gt;算法的步骤如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911142423655.png&#34;
	width=&#34;1005&#34;
	height=&#34;1271&#34;
	srcset=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911142423655_hu24faa5cb662afb2dcb93b62ad6b0e1c4_179576_480x0_resize_box_3.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230911142423655_hu24faa5cb662afb2dcb93b62ad6b0e1c4_179576_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;BLPA算法&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;79&#34;
		data-flex-basis=&#34;189px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化：将每个节点随机分配给k个分片之一&lt;/li&gt;
&lt;li&gt;重新分配配置文件计算：对于每个节点$u$，使用元组$\left\langle u, \mathbb{C}_{s r c}, \mathbb{C}_{d s t}, \xi\right\rangle$表示其重新分配的配置文件，其中$\mathbb{C}_{s r c}$和$\mathbb{C}_{d s t}$是节点$u$的当前分片和目标分片，$\xi$是目标分片$\mathbb{C}_{d s t}$的邻居计数，并将其存入$\mathbb{F}$&lt;/li&gt;
&lt;li&gt;排序：邻居数量越多的重新分配配置文件应具有越高的优先级，所以按$\xi$对$\mathbb{F}$进行降序排序&lt;/li&gt;
&lt;li&gt;传播标签：枚举$\mathbb{F}$里的所有元素，如果$\mathbb{C}_{d s t}$的大小不超过给定的阈值$\delta$，就将其添加到目标分片并从当前分片中删除。之后在$\mathbb{F}$中删除所有剩余的包含节点$u$的元组。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;之后不断迭代，直到分片不更改或达到最大迭代$T$&lt;/p&gt;
&lt;p&gt;算法的时间复杂度为$O(n·d_{ave})$，$n$为节点数，$d_{ave}$为训练图的平均节点数。&lt;/p&gt;
&lt;p&gt;作者无法从理论上证明其收敛性，不过通过实验表示$T=30$时几乎是收敛的。&lt;/p&gt;
&lt;h3 id=&#34;嵌入式聚类算法&#34;&gt;嵌入式聚类算法&lt;/h3&gt;
&lt;p&gt;对于策略2，作者使用预训练的GNN模型来获取所有节点嵌入，然后对生成的节点嵌入执行聚类。&lt;/p&gt;
&lt;p&gt;思路是将GNN模型的所有节点投影到空间中，再使用K-Means进行聚类。同样也会导致分块的不平均这个问题。&lt;/p&gt;
&lt;p&gt;对此，作者提出了Balanced Embedding k-means (BEKM)。定义preference「偏好值」为节点嵌入和所有节点分片对的分片质心之间的欧氏距离。&lt;/p&gt;
&lt;p&gt;具体的算法如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912153952657.png&#34;
	width=&#34;998&#34;
	height=&#34;1447&#34;
	srcset=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912153952657_hudb2407a56f2f4bb52207cf59dc47330a_197080_480x0_resize_box_3.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912153952657_hudb2407a56f2f4bb52207cf59dc47330a_197080_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;BEKM算法&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;68&#34;
		data-flex-basis=&#34;165px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化：随机选择$k$个质心$C^0=\{C^0_1,C^0_2,\dots,C^0_k\}$&lt;/li&gt;
&lt;li&gt;计算嵌入质心距离：计算节点嵌入和质心之间的所有成对距离，从而得到$n\times k$个嵌入质心对。这些对存储在$\mathbb{F}$中。&lt;/li&gt;
&lt;li&gt;排序质心距离：距离较近的嵌入质心对具有更高的优先级，所以按照升序对$\mathbb{F}$进行排序&lt;/li&gt;
&lt;li&gt;重新分配节点和更新质心：枚举$\mathbb{F}$里的所有元素，如果$\mathbb{C}_{j}$的大小不超过给定的阈值$\delta$，就将其添加到目标分片。之后在$\mathbb{F}$中删除所有剩余的包含节点$i$的元组。最后，将新质心计算为其相应分片中所有节点的平均值。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;同样，不断重复直到分片不更改或达到最大迭代$T$&lt;/p&gt;
&lt;p&gt;算法的时间复杂度为$O(k·n)$，$n$个节点,$k$个分片。&lt;/p&gt;
&lt;h2 id=&#34;基于学习的聚合&#34;&gt;基于学习的聚合&lt;/h2&gt;
&lt;p&gt;目前常见的聚合方式有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MajAggr：每个分片模型预测一个标签，取最多预测的标签&lt;/li&gt;
&lt;li&gt;MeanAggr：收集所有分片模型的后验向量，然后求平均值，得到聚合后验，选取最高后验值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者提出了一种基于学习的聚合方法LBAggr，为每个分片模型分配一个重要性分数，通过以下损失函数进行学习：&lt;/p&gt;
&lt;p&gt;$$\min _{\alpha} \underset{w \in \mathcal{G}_{o}}{\mathbb{E}}\left[\mathcal{L}\left(\sum_{i=0}^{m} \alpha_{i} \cdot \mathcal{F}_{i}\left(X_{w}, \mathcal{N}_{w}\right), y\right)\right]+\lambda \sum_{i=0}^{m}\left|\alpha_{i}\right|$$&lt;/p&gt;
&lt;p&gt;其中$X_{w}$和$\mathcal{N}_{w}$是训练图中节点$w$的特征向量和邻域，$y$是$w$的真实标签，$\mathcal{F}_{i}(\cdot)$表示分片模型$i$，$\alpha_{i}$是$\mathcal{F}_{i}(\cdot)$的重要性得分，$m$是分片总数。将所有重要性分数的总和调节为 1。&lt;/p&gt;
&lt;p&gt;作者通过梯度下降来找到最优的$\alpha$，从而解决最优化问题。然而，直接梯度下降会导致$\alpha$为负数。为了解决此问题，作者使用softmax 函数在每次迭代中进行归一化处理。&lt;/p&gt;
&lt;p&gt;为了提升运行速度，作者指出可以使用训练图中 10% 的节点进行重新训练。&lt;/p&gt;
&lt;h2 id=&#34;grapheraser&#34;&gt;GraphEraser&lt;/h2&gt;
&lt;p&gt;将上面提到的方法集合在一起，就得到了此算法。当某些节点或边缘被数据所有者撤销时，只需要重新训练相应的分片模型即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912202314840.png&#34;
	width=&#34;1029&#34;
	height=&#34;1020&#34;
	srcset=&#34;https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912202314840_huf90398ba628bfbb987f17970ca93841e_168537_480x0_resize_box_3.png 480w, https://lbqaq.top/p/graph-unlearning/IMAGE/image-20230912202314840_huf90398ba628bfbb987f17970ca93841e_168537_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GraphEraser&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;242px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;评估&#34;&gt;评估&lt;/h2&gt;
&lt;p&gt;🕊️鸽了&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
