<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数据挖掘 on luoboQAQ</title>
    <link>https://lbqaq.top/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/</link>
    <description>Recent content in 数据挖掘 on luoboQAQ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 10 May 2022 14:00:16 +0800</lastBuildDate><atom:link href="https://lbqaq.top/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>分类算法学习笔记</title>
      <link>https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 10 May 2022 14:00:16 +0800</pubDate>
      
      <guid>https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>分类算法的作用 分类是在一群已经知道类别标号的样本中，训练一种分类器，让其能够对某种未知的样本进行分类。分类算法属于一种有监督的学习。分类算法的分类过程就是建立一种分类模型来描述预定的数据集或概念集，通过分析由属性描述的数据库元组来构造模型。分类的目的就是使用分类对新的数据集进行划分，其主要涉及分类规则的准确性、过拟合、矛盾划分的取舍等。
 有监督学习和无监督学习的区别  有监督学习是指数据集的正确输出已知情况下的一类学习算法。因为输入和输出已知，意味着输入和输出之间有一个关系，监督学习算法就是要发现和总结这种“关系”。 无监督学习是指对无标签数据的一类学习算法。因为没有标签信息，意味着需要从数据集中发现和总结模式或者结构。 简单来说，是否有监督（supervised），就看输入数据是否有标签（label）    交叉验证 基本思想 交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组，一部分做为训练集(train set)，另一部分做为验证集(validation set or test set)，首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型(model)，以此来做为评价分类器的性能指标。用交叉验证的目的是为了得到可靠稳定的模型。
K折交叉验证(K-fold cross-validation) K折交叉验证就是进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练、测试评估，从而得出一个评价结果；如果是10折交叉验证，意思就是在原始数据集上，进行10次划分，每次划分进行一次训练、评估，最后得到10次划分后的评估结果，一般在这几次评估结果上取平均得到最后的评分。其中，k一般取5或10。
K折交叉验证的步骤：
 将原始数据集划分为相等的K部分（“折”） 将第1部分作为测试集，其余作为训练集 训练模型，计算模型在测试集上的准确率 每次用不同的部分作为测试集，重复步骤2和3 K次 将平均准确率作为最终的模型准确率   10折交叉验证 
 💡 要会给定数据集进行K折交叉验证 要会计算模型准确率（每次的准确率、最终的准确率）
 支持向量机(SVM) 基本思想 SVM学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。
 SVM基本思想 
用我自己的理解，就是找一条线将两个数据集分开，要保证这条线到两边的数据集距离最大。那么，如何才能保证距离最大呢，这就是下面要讨论的难点了。
支持向量  在支持向量机中，距离超平面最近的且满足一定条件的几个训练样本点被称为支持向量。
 在上图中，处于虚线上的点（即红色的点）我们就将其定义为支持向量。那么，如何得到支持向量到超平面的距离呢？我们引入几何间隔的概念：
$$ \gamma = \frac{y(w^Tx + b)}{||w||_2} = \frac{\gamma^{&amp;rsquo;}}{||w||_2} $$
一般我们都取函数间隔$\gamma^{&amp;rsquo;}$为1，这样我们就可以得到支持向量到超平面的距离为$\frac{1}{||w||_2}$，两个支持向量之间的距离为$\frac{2}{||w||_2}$
SVM模型目标函数 SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：
$$ max \;\; \frac{1}{||w||_2} \;\; s.t \;\; y_i(w^Tx_i + b) \geq 1 (i =1,2,&amp;hellip;m) $$</description>
    </item>
    
    <item>
      <title>聚类算法学习笔记</title>
      <link>https://lbqaq.top/p/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 10 May 2022 08:23:41 +0800</pubDate>
      
      <guid>https://lbqaq.top/p/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>Kmeans 算法步骤  从样本中选择 K 个点作为初始质心（完全随机） 计算每个样本到各个质心的距离，将样本划分到距离最近的质心所对应的簇中 计算每个簇内所有样本的均值，并使用该均值更新簇的质心 重复步骤 2 与 3 ，直到达到以下条件之一：  质心的位置变化小于指定的阈值（默认为 0.0001） 达到最大迭代次数    欧氏距离  衡量的是多维空间中两个点之间的绝对距离
 在二维和三维空间中的欧氏距离就是两点之间的实际距离
计算方法：对应坐标值相减的平方和再开方
$$ dist(X,Y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2} $$
对于二维来说，就是
$$ dist(X,Y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2} $$
注意理解输入？输出？类别中心？迭代过程中做什么？
编程实现 %% 生成测试数据 clear p=100; % 每簇的样本数 sigma = [0.1 0;0 0.1]; % 协方差矩阵 R = chol(sigma); % 生成测试数据集 data=[... [0 0] + randn(p,2)*R,1*ones(p,1);... [0 2] + randn(p,2)*R,2*ones(p,1);... [2 0] + randn(p,2)*R,3*ones(p,1);... [2 2] + randn(p,2)*R,4*ones(p,1);... ]; %% 绘制样本散点图 figure(1) scatter(data(:,1),data(:,2),&amp;#39;r.</description>
    </item>
    
  </channel>
</rss>
