<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数据挖掘 on luoboQAQ</title>
    <link>https://lbqaq.top/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/</link>
    <description>Recent content in 数据挖掘 on luoboQAQ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 17 May 2022 22:45:18 +0800</lastBuildDate><atom:link href="https://lbqaq.top/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>数据可视化笔记</title>
      <link>https://lbqaq.top/p/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 17 May 2022 22:45:18 +0800</pubDate>
      
      <guid>https://lbqaq.top/p/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AC%94%E8%AE%B0/</guid>
      <description>主成分分析(PCA) 基本思想 PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据$(x^{(1)},x^{(2)},&amp;hellip;,x^{(m)})$。我们希望将这m个数据的维度从n维降到n&amp;rsquo;维，希望这m个n&amp;rsquo;维的数据集尽可能的代表原始数据集。
通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。
由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法。
特征值分解(EVD) 特征值和特征向量的定义如下：
$$ Ax=\lambda x $$
其中A是一个$n \times n$的实对称矩阵，$x$是一个$n$维向量，则我们说$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$所对应的特征向量。
求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的$n$个特征值$\lambda_1 \leq \lambda_2 \leq &amp;hellip; \leq \lambda_n$,以及这$n$个特征值所对应的特征向量${q_1,q_2,&amp;hellip;q_n}$，如果这$n$个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：$A=Q\Sigma Q^{-1}$ 。
其中，Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵A的信息可以由其特征值和特征向量表示。
注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。
奇异值分解(SVD) SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个$m \times n$的矩阵，那么我们定义矩阵A的SVD为：
$$ A = U\Sigma V^T $$
其中U是一个$m \times m$的矩阵，$\Sigma$是一个$m \times n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个$n \times n$的矩阵。
计算方法：
 我们将A的转置和A做矩阵乘法，那么会得到$n \times n$的一个方阵$A^TA$。对其进行特征值分解，得到n个特征值和对应的n个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n \times n$的矩阵V，就是我们SVD公式里面的V矩阵了。 对$AA^T$进行特征值分解，得到m个特征值和对应的m个特征向量$u$了。将$AA^T$的所有特征向量张成一个$m \times m$的矩阵U，就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。 由于奇异值矩阵$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\sigma$就可以了。求解奇异值可以用$\sigma_i = Av_i / u_i$或$\sigma_i = \sqrt{\lambda_i}$  算法过程 输入：n维样本集$D=(x^{(1)}, x^{(2)},&amp;hellip;,x^{(m)})$，要降维到的维数n&#39;.
输出：降维后的样本集$D&#39;$
 对所有的样本进行中心化： $x^{(i)} = x^{(i)} - \frac{1}{m}\sum\limits_{j=1}^{m} x^{(j)}$ 计算样本的协方差矩阵$XX^T$ 对矩阵$XX^T$进行特征值分解 取出最大的n&amp;rsquo;个特征值对应的特征向量$(w_1,w_2,&amp;hellip;,w_{n&amp;rsquo;})$, 将所有的特征向量标准化后，组成特征向量矩阵W。 对样本集中的每一个样本$x^{(i)}$,转化为新的样本$z^{(i)}=W^Tx^{(i)}$ 得到输出样本集$D&amp;rsquo; =(z^{(1)}, z^{(2)},&amp;hellip;,z^{(m)})$  上面是采用EVD，如果采用SVD，在第二第三步时就用SVD进行解决。</description>
    </item>
    
    <item>
      <title>分类算法学习笔记</title>
      <link>https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 10 May 2022 14:00:16 +0800</pubDate>
      
      <guid>https://lbqaq.top/p/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>分类算法的作用 分类是在一群已经知道类别标号的样本中，训练一种分类器，让其能够对某种未知的样本进行分类。分类算法属于一种有监督的学习。分类算法的分类过程就是建立一种分类模型来描述预定的数据集或概念集，通过分析由属性描述的数据库元组来构造模型。分类的目的就是使用分类对新的数据集进行划分，其主要涉及分类规则的准确性、过拟合、矛盾划分的取舍等。
 有监督学习和无监督学习的区别  有监督学习是指数据集的正确输出已知情况下的一类学习算法。因为输入和输出已知，意味着输入和输出之间有一个关系，监督学习算法就是要发现和总结这种“关系”。 无监督学习是指对无标签数据的一类学习算法。因为没有标签信息，意味着需要从数据集中发现和总结模式或者结构。 简单来说，是否有监督（supervised），就看输入数据是否有标签（label）    交叉验证 基本思想 交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组，一部分做为训练集(train set)，另一部分做为验证集(validation set or test set)，首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型(model)，以此来做为评价分类器的性能指标。用交叉验证的目的是为了得到可靠稳定的模型。
K折交叉验证(K-fold cross-validation) K折交叉验证就是进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练、测试评估，从而得出一个评价结果；如果是10折交叉验证，意思就是在原始数据集上，进行10次划分，每次划分进行一次训练、评估，最后得到10次划分后的评估结果，一般在这几次评估结果上取平均得到最后的评分。其中，k一般取5或10。
K折交叉验证的步骤：
 将原始数据集划分为相等的K部分（“折”） 将第1部分作为测试集，其余作为训练集 训练模型，计算模型在测试集上的准确率 每次用不同的部分作为测试集，重复步骤2和3 K次 将平均准确率作为最终的模型准确率   10折交叉验证 
 💡 要会给定数据集进行K折交叉验证 要会计算模型准确率（每次的准确率、最终的准确率）
 支持向量机(SVM) 基本思想 SVM学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。
 SVM基本思想 
用我自己的理解，就是找一条线将两个数据集分开，要保证这条线到两边的数据集距离最大。那么，如何才能保证距离最大呢，这就是下面要讨论的难点了。
支持向量  在支持向量机中，距离超平面最近的且满足一定条件的几个训练样本点被称为支持向量。
 在上图中，处于虚线上的点（即红色的点）我们就将其定义为支持向量。那么，如何得到支持向量到超平面的距离呢？我们引入几何间隔的概念：
$$ \gamma = \frac{y(w^Tx + b)}{||w||_2} = \frac{\gamma^{&amp;rsquo;}}{||w||_2} $$
一般我们都取函数间隔$\gamma^{&amp;rsquo;}$为1，这样我们就可以得到支持向量到超平面的距离为$\frac{1}{||w||_2}$，两个支持向量之间的距离为$\frac{2}{||w||_2}$
SVM模型目标函数 SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为：
$$ max \;\; \frac{1}{||w||_2} \;\; s.t \;\; y_i(w^Tx_i + b) \geq 1 (i =1,2,&amp;hellip;m) $$</description>
    </item>
    
    <item>
      <title>聚类算法学习笔记</title>
      <link>https://lbqaq.top/p/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 10 May 2022 08:23:41 +0800</pubDate>
      
      <guid>https://lbqaq.top/p/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>Kmeans 算法步骤  从样本中选择 K 个点作为初始质心（完全随机） 计算每个样本到各个质心的距离，将样本划分到距离最近的质心所对应的簇中 计算每个簇内所有样本的均值，并使用该均值更新簇的质心 重复步骤 2 与 3 ，直到达到以下条件之一：  质心的位置变化小于指定的阈值（默认为 0.0001） 达到最大迭代次数    欧氏距离  衡量的是多维空间中两个点之间的绝对距离
 在二维和三维空间中的欧氏距离就是两点之间的实际距离
计算方法：对应坐标值相减的平方和再开方
$$ dist(X,Y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2} $$
对于二维来说，就是
$$ dist(X,Y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2} $$
注意理解输入？输出？类别中心？迭代过程中做什么？
编程实现 %% 生成测试数据 clear p=100; % 每簇的样本数 sigma = [0.1 0;0 0.1]; % 协方差矩阵 R = chol(sigma); % 生成测试数据集 data=[... [0 0] + randn(p,2)*R,1*ones(p,1);... [0 2] + randn(p,2)*R,2*ones(p,1);... [2 0] + randn(p,2)*R,3*ones(p,1);... [2 2] + randn(p,2)*R,4*ones(p,1);... ]; %% 绘制样本散点图 figure(1) scatter(data(:,1),data(:,2),&amp;#39;r.</description>
    </item>
    
  </channel>
</rss>
